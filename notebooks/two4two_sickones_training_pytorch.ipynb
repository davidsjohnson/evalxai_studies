{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1n5452htjSn6n7eCmFVkp2VlO7Y3M8b8Z",
      "authorship_tag": "ABX9TyMgb1FCqZA1tto1ZQWY8op9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsjohnson/evalxai_studies/blob/main/notebooks/two4two_sickones_training_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi0pAex6RNDU"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "2cRrJspk1gmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Params"
      ],
      "metadata": {
        "id": "rxTk1ctGpvnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = ['Healthy', 'OC Degeneration']\n",
        "\n",
        "load_checkpoints = False\n",
        "continue_training = False\n",
        "\n",
        "modeltype = 'mobilenet'\n",
        "\n",
        "ds = 'sick_ones_bendbias'\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.0001"
      ],
      "metadata": {
        "id": "SLpJb-H9pxs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Load Datasets"
      ],
      "metadata": {
        "id": "Ua-n5ADsde6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relative_model_path = \"two4two_sickones_models_pytorch\"\n",
        "base_path = Path('/content/drive/MyDrive') / relative_model_path\n",
        "base_path"
      ],
      "metadata": {
        "id": "eSBMiJ-ARVcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data downloading and dataset utilities\n",
        "\n",
        "def download_file(url, file_name, cache_dir=\"data\", extract=True, force_download=False, archive_folder=None):\n",
        "    # Ensure the cache directory exists\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    file_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "    # Download the file\n",
        "    if not os.path.exists(file_path) or force_download:\n",
        "      torch.hub.download_url_to_file(url, file_path)\n",
        "      print(f\"File downloaded to: {file_path}\")\n",
        "    else:\n",
        "      print(f\"File already exists at: {file_path}\")\n",
        "\n",
        "    if extract:\n",
        "      with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=cache_dir)\n",
        "      print(f\"File extracted to: {cache_dir}\")\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "\n",
        "    return Path(file_path)\n",
        "\n",
        "def load_dataframe(data_dir, dataset):\n",
        "  data_dir = data_dir / dataset\n",
        "  df = pd.read_json(data_dir / 'parameters.jsonl', lines=True)\n",
        "  df['filename'] = df['id'] + '.png'\n",
        "  df['ill'] = df['ill'].astype(int).astype(str)\n",
        "  return df\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.df.iloc[idx]['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = int(self.df.iloc[idx]['ill'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "HPi0wvbicrct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset and Dataloaders\n"
      ],
      "metadata": {
        "id": "ReNR_Y0QProQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download datafrom sciebo\n",
        "data_dir = download_file(\"https://uni-bielefeld.sciebo.de/s/AIJLvXMwP0ngEiW/download\",\n",
        "                         \"two4two_datasets.tar.gz\",\n",
        "                         cache_dir='/content/data',\n",
        "                         extract=True,\n",
        "                         force_download=False,\n",
        "                         archive_folder='two4two_datasets')\n",
        "data_dir"
      ],
      "metadata": {
        "id": "6a1B47gWYgVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dir = data_dir / ds\n",
        "ds_dir"
      ],
      "metadata": {
        "id": "bn3D8znPQN-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets and dataloaders for Training and Evaluation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "val_df = load_dataframe(ds_dir, 'validation')\n",
        "test_df = load_dataframe(ds_dir, 'test')\n",
        "\n",
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                              num_workers=6, pin_memory=True)\n",
        "\n",
        "val_dataset = ImageDataset(val_df,  ds_dir / 'validation', transform=transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                            num_workers=6, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageDataset(test_df,  ds_dir / 'test', transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)"
      ],
      "metadata": {
        "id": "n-yCAoLQprGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "LJrJOTfzsIof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "I4f0lOt3tap0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis Dataset"
      ],
      "metadata": {
        "id": "pV5OA2GDS8nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create column for absolute sphere difference\n",
        "train_df['sphere_diff'] = np.abs(train_df['spherical'] - train_df['ill_spherical'])\n",
        "val_df['sphere_diff'] = np.abs(val_df['spherical'] - val_df['ill_spherical'])\n",
        "test_df['sphere_diff'] = np.abs(test_df['spherical'] - test_df['ill_spherical'])"
      ],
      "metadata": {
        "id": "mpOmqqdxsLBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Data\n"
      ],
      "metadata": {
        "id": "Rfg6TYXeTgkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g =sns.jointplot(data=train_df, x='spherical', y='ill_spherical', hue='ill', s=5, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g =sns.jointplot(data=train_df, x='spherical', y='sphere_diff', hue='ill', s=5, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g = sns.jointplot(data=train_df, x='obj_color', y='sphere_diff', hue='ill', s=5, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g = sns.jointplot(data=train_df, x='bending', y='sphere_diff', hue='ill', s=5, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)"
      ],
      "metadata": {
        "id": "E9dBICcG8H10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Data"
      ],
      "metadata": {
        "id": "1X5FuMebTqAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g =sns.jointplot(data=test_df, x='spherical', y='ill_spherical', hue='ill', s=15, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g =sns.jointplot(data=test_df, x='spherical', y='sphere_diff', hue='ill', s=15, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g = sns.jointplot(data=test_df, x='obj_color', y='sphere_diff', hue='ill', s=15, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)\n",
        "g = sns.jointplot(data=test_df, x='bending', y='sphere_diff', hue='ill', s=15, alpha=0.5)\n",
        "sns.move_legend(g.ax_joint, \"upper left\", title='Ill', frameon=True)"
      ],
      "metadata": {
        "id": "zvqCyDxDmNyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "AiMdu2w_dj2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mobilenetv2(num_classes, pretrained=True, checkpoint_path=None):\n",
        "  \"\"\"Loads a MobileNetV2 model, optionally loading from a checkpoint.\n",
        "\n",
        "  Args:\n",
        "    num_classes: The number of output classes.\n",
        "    pretrained: Whether to load the pre-trained weights.\n",
        "    checkpoint_path: Path to a checkpoint file to load.\n",
        "\n",
        "  Returns:\n",
        "    A MobileNetV2 model.\n",
        "  \"\"\"\n",
        "  model = models.mobilenet_v2(weights=None if not pretrained else 'DEFAULT')\n",
        "  model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
        "\n",
        "  if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hH-LE1Vlspbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Sets random seeds for reproducibility.\n",
        "  \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}, Evaluation Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_model(model, dl_train, dl_val, criterion, optimizer, scheduler, device, checkpoint_path, num_epochs=10):\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  best_val_loss = sys.float_info.max\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in dl_train:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track training loss and accuracy\n",
        "        running_train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_loss = running_train_loss / len(dl_train)\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dl_val:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track validation loss and accuracy\n",
        "            running_val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / len(dl_val)\n",
        "    val_accuracy = correct_val / total_val\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"\\tValidation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Checkpointing the best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        print(f\"New best model found at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), checkpoint_path / 'tmp' / 'best_model.pth')  # Save only the model's state_dict\n",
        "\n",
        "  # To load the best model later:\n",
        "  model = load_mobilenetv2(num_classes=len(CLASSES),\n",
        "                           pretrained=False,\n",
        "                           checkpoint_path=checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "  model.to(device)\n",
        "\n",
        "  val_loss, val_acc = evaluate_model(model, dl_val, criterion, device)\n",
        "\n",
        "  print(f\"Training Run complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f}\", )\n",
        "  print(\"-\" * 30)\n",
        "\n",
        "  return model, val_loss, val_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "fd7Ww-9otfaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "vFQ2mJdoV7v-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msUfBSYCRzNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup model path\n",
        "model_path = base_path / ds / f'{modeltype}'\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Model path:\", model_path)\n",
        "\n",
        "# setup checkpoint folders\n",
        "checkpoint_path = model_path / \"torch_mobilenetv2/\"\n",
        "(checkpoint_path / 'tmp').mkdir(parents=True, exist_ok=True)\n",
        "(checkpoint_path / 'final').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "k7ViedtsuqzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run training\n",
        "\n",
        "n_runs = 1\n",
        "n_epochs = 10\n",
        "\n",
        "best_val_loss = sys.float_info.max\n",
        "for i in range(n_runs):\n",
        "\n",
        "  set_seed(42 + i)\n",
        "\n",
        "  print(f\"Run {i+1} / {n_runs}\")\n",
        "  print(\"=\" * 30)\n",
        "\n",
        "  model = load_mobilenetv2(len(CLASSES), pretrained=False, checkpoint_path=None)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=True)\n",
        "\n",
        "  # Scheduler\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=4,\n",
        "                                                          threshold=0.01, threshold_mode='abs', verbose=True)\n",
        "\n",
        "  model, val_loss, val_acc = train_model(model,\n",
        "                                         train_dataloader, val_dataloader,\n",
        "                                         criterion, optimizer, scheduler,\n",
        "                                         device, checkpoint_path,\n",
        "                                         num_epochs=n_epochs)\n",
        "\n",
        "  # Checkpointing the best model\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      print(f\"New best model found at Run {i+1} with validation loss: {val_loss:.4f}\")\n",
        "      torch.save(model.state_dict(), checkpoint_path / 'final' / 'best_model.pth')  # Save only the model's state_dict\n",
        "  print()\n",
        "\n",
        "# Load best model:\n",
        "model = load_mobilenetv2(num_classes=len(CLASSES),\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "\n",
        "val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\n",
        "\n",
        "print(f\"Training complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f}\", )\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "GKfOi8bpu90P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model"
      ],
      "metadata": {
        "id": "T-gxat5Ju4zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model and evaluate\n",
        "model = load_mobilenetv2(num_classes=len(CLASSES),\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "\n",
        "evaluate_model(model, train_dataloader, criterion, device)\n",
        "evaluate_model(model, val_dataloader, criterion, device)\n",
        "evaluate_model(model, test_dataloader, criterion, device)"
      ],
      "metadata": {
        "id": "s86_4TcNxj-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSuYGeCN_e8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}